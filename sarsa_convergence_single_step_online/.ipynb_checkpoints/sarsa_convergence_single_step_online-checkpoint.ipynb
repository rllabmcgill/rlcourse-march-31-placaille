{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence and the exploration requirements for Sarsa(0)\n",
    "\n",
    "In this assignment I cover the article *Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms* by [Singh et al](http://link.springer.com/article/10.1023%2FA%3A1007678930559). It tackles the exploration/exploitation tradeoff and provides proof as well as suggestions for ensuring convergence for On-Policy Sarsa(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "One of the most typical problems with reinforcement algorithms is balancing between exploration and exploitation of actions and states. It comes down to an important consideration that can greatly affect the outcome of the training algorithms. This quote in  [Singh et al](http://link.springer.com/article/10.1023%2FA%3A1007678930559) sums it best.\n",
    "\n",
    "> All such algorithms face a tradeoff between exploitation and exploration [...] i.e., between choosing actions that are best according to the current state of knowledge, and actions that are not the current best but improve the state of knowledge and potentially yield a higher payoff in the future.\n",
    "\n",
    "This problem is especially important during control as the policy is trying to be optimized. The article briefly mentions that off-policy learning algorithms, such as Q-Learning, conceptually seperate the exploration component from the actual control that is being made, as the policy where control is applied is not the one used for exploration and therefore have less of a problem.\n",
    "\n",
    "The authors argue that for on-policy online learning algorithms (such as Sarsa(0)) some conditions are necessary to ensure convergence with regards to the level of exploration, in particular because the update rule to the value functions actually depends on the actions that are derived from that same value function.\n",
    "\n",
    "They also argue that the analysis is particularily important for on-policy algorithms because of the direct consequences exploration has on the actual outcome. Indeed, since exploration is intertwined with actual actions taken, it is riskier for the agent and therefore an appropriate exploration schedule is essential. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration learning policies\n",
    "\n",
    "The article presentes two types of exploration policies, the *decaying* and *persistent* exploration learning policies.\n",
    "\n",
    "They argue that both these methods have their advantage. The persistent exploration ensures a level of adaptability forever as it is desgned to keep the same level of exploration forever. However, this introduces a problem where the policy in the limit will not converge perfectly to the optimal, even though it could, since there will still be within a margin.\n",
    "\n",
    "This is where the decaying exploration has its advantage of having the possibility to get closer to the optimal policy during control. However, this has the disadvantage that as the exploration decays, it will be more difficult to adapt. This is an important issue with this method that can be somewhat managed by making the exploration a function of the number of visits to a state and/or state-action pair. This is explored later in the article.\n",
    "\n",
    "For the purpose of this assignment, I will be covering only the decaying exploration methods which according to the authors need the following two properties to proof convergence:\n",
    "\n",
    "> 1. each action is executed infinitely often in every state that is visited infinitely often, and\n",
    "2. in the limit, the learning policy is greedy with respect to the Q-value function with probability 1.\n",
    "\n",
    "They classify learning policies that fit these properties as methods that are *greedy in the limite with infinite exploration* (GLIE). For the remainder of this assignment, only GLIE learning policies will be explored/analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence with decaying exploration\n",
    "\n",
    "Following the two elements above mentioned that constitute a GLIE learning policy, the previous convergence theorems (the article points out to Dayan & Sejnowski, 1994; Jaakkola et al., 1994; Tsitsiklis, 1994; Szepesvari & Littman, 1996) cannot be directly applied due to the fact that Sarsa(0) is an on-policy learning algorithm.\n",
    "\n",
    "The authors present the following lemma\n",
    "\n",
    "<img src=\"lemma_1.png\">\n",
    "\n",
    "If we consider $x_t$ in the above lemma as being our state-action pair $(s_t, a_t)$ and $\\Delta_t$ as being $Q_t - Q^*$, we notice that we are therefore using this to show that the difference between the current Q-value and the optimal value will converge to 0 under the tabular case.\n",
    "\n",
    "The authors prove this lemma in the appendix A of their paper.\n",
    "\n",
    "They then use this lemma to prove the convergence of Sarsa(0), stating the following theorem.\n",
    "\n",
    "<img src=\"thm_1.png\">\n",
    "\n",
    "This theorem breaks down the convergence of the Sarsa(0) algorithm to the optimal Q-values and optimal policy, by simply ensuring that the the learning policy is GLIE, and the three other conditions listed are satisfied. In addition, the listed conditions are very reasonable if only tabular cases are considerd.\n",
    "\n",
    "#### Proof of theorem\n",
    "\n",
    "As given by lemma 1 and by using $x_t$ as $(s_t, a_t)$, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta_{t+1} (s_t, a_t) = (1-\\alpha_t(s_t, a_t)) \\Delta_{t} (s_t, a_t) + \\alpha_t(s_t, a_t) F_t(s_t, a_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where under Sarsa(0),\n",
    "$$\n",
    "\\begin{align}\n",
    "F_t(s_t, a_t) &= r_t + \\gamma \\max_{b \\in A} Q_t(s_{t+1}, b) - Q^*(s_t, a_t) + \\gamma \\Big[ Q_t(s_{t+1}, a_{t+1}) - \\max_{b \\in A} Q_t(s_{t+1}, b) \\Big] \\\\\n",
    "&= r_t + \\gamma \\max_{b \\in A} Q_t(s_{t+1}, b) - Q^*(s_t, a_t) + C_t(Q) \\\\\n",
    "&= F^Q_t(s_t, a_t) + C_t(s_t, a_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The authors also point out that if it was the Q-leaning algorithm that was being analyzes, $F_t(s_t, a_t)$ would be $F^Q_t(s_t, a_t)$.\n",
    "\n",
    "We will therefore be able to use some properties from the Q-learning algorithms to prove convergence for Sarsa(0). The only main difference is that we will need to consider the term $C_t(s_t, a_t)$ that is added.\n",
    "\n",
    "They mention that given Q-learning known fact that $||E\\{F^Q_t(\\cdot, \\cdot)|P_t \\}|| \\le \\gamma ||\\Delta_t||$ for all $t$.\n",
    "\n",
    "Given that Q-learning property and previous result w.r.t. to $F_t(s_t, a_t)$ we can further expand,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "||E \\{ F_t (\\cdot, \\cdot) | P_t \\}  &\\le ||E\\{F^Q_t(\\cdot, \\cdot)|P_t \\}|| + ||E\\{C_t(\\cdot, \\cdot)|P_t \\}|| \\\\\n",
    "&\\le \\gamma || \\Delta_t || + ||E\\{C_t(\\cdot, \\cdot)|P_t \\}||\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By looking back at lemma 1, point 3, we can notice that if we consider $||E\\{C_t(\\cdot, \\cdot)|P_t \\}|| = c_t$, if we can prove that $c_t$ converges to zero with probability 1, then the proof will be complete as we can apply the lemma.\n",
    "\n",
    "The authors then brielfy mention that this indeed converges to zero as per the different assumptions made on the GLIE and the environment being a MDP as well Sarsa(0) being bounded by Q-learning (by using max and min operators).\n",
    "\n",
    "It is also argued that this provides even more strenght to the Q-learning algorithm since applying a GLIE learning policy with Q-learning would also make the learned policy to converge to the optimal one, not just the optimal Q-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements for decaying exploration strategies\n",
    "\n",
    "Two of the most well known exploration strategies are the $\\epsilon$-greedy and Boltzmann learning policies. In this section I will cover the requirements for these two methods in order to be used with Sarsa(0) that ensure convergence under the tabular case.\n",
    "\n",
    "In order to ensure convergence, these strategies need to have the properties detailed before that can qualify them as GLIE, greedy in the limit with infinite exploration.\n",
    "\n",
    "We therefore need to show for both methods that $\\sum \\limits_{i=1}^{\\infty} Pr(a|s, t_s(i)) = \\infty$ and $\\lim_{t\\rightarrow \\infty} exploration = 0$.\n",
    "\n",
    "#### Boltzmann exploration\n",
    "\n",
    "Under the Boltzmann exploration, the policy for selecting an action, for a given state $s$, at time $t$ and $n_t(s)$, where $n_t(s)$ denotes the number of time the state $s$ has been visited up to time $t$, is given by the following.\n",
    "\n",
    "$Pr(a|s, t, Q, n_t(s)) = \\dfrac{e^{\\beta_t(s)Q(s,a)}}{\\sum_{b\\in A}e^{\\beta_t(s)Q(s,b)}}$\n",
    "\n",
    "In addition, given $\\sum_{b \\in A}^{\\infty} c / i = \\infty$ we need for all $a$, $\\beta_t(s)$ that satisfies,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dfrac{e^{\\beta_t(s)Q(s,a)}}{\\sum_{b\\in A}e^{\\beta_t(s)Q(s,b)}} &\\ge \\dfrac{c}{n_t(s)} &\\\\\n",
    "n_t(s) e^{\\beta_t(s)Q(s,a)} &\\ge c \\sum_{b\\in A} e^{\\beta_t(s)Q(s,b)} &\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The summation can be upper bounded by $m$ times the highest Q-value, where $m$ is the number of actions, we have,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "n_t(s) e^{\\beta_t(s)Q(s,a)} &\\ge c \\sum_{b\\in A} e^{\\beta_t(s)Q(s,b)} & \\\\\n",
    "n_t(s) e^{\\beta_t(s)Q(s,a)} &\\ge c m e^{\\beta_t(s)Q(s,b_{max})} & \\\\\n",
    "\\dfrac{n_t(s)}{cm} &\\ge e^{\\beta_t(s)(Q(s,b_{max})-Q(s,a))} & \\\\\n",
    "\\log n_t(s) - \\log cm & \\ge \\beta_t(s)(Q(s,b_{max})-Q(s,a)) & \\\\\n",
    "\\dfrac{\\log n_t(s) - \\log cm}{Q(s,b_{max})-Q(s,a)} &\\ge \\beta_t(s)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This can be simplified by taking $c = 1/m$, and we therefore obtain, $\\beta_t(s) \\le \\dfrac{\\log n_t(s)}{Q(s,b_{max})-Q(s,a)}$.\n",
    "\n",
    "In addition, we need to ensure that it is greedy in the limit to make it GLIE.\n",
    "\n",
    "By selecting $\\beta_t(s) = \\dfrac{\\log n_t(s)}{Q(s,b_{max})-Q(s,a)}$, we have $\\lim_{t\\rightarrow \\infty} \\beta_t(s) = \\lim_{t\\rightarrow \\infty} \\dfrac{\\log n_t(s)}{Q(s,b_{max})-Q(s,a)} = \\infty$.\n",
    "\n",
    "This makes the probability of selecting the most valued action converge to 1 in the limit. All of the conditions to qualify as a GLIE are therefore met.\n",
    "\n",
    "#### $\\epsilon$-greedy exploration\n",
    "\n",
    "Under $\\epsilon$-greedy exploration, a random action is selected with probability $\\epsilon_t(s)$, while the highest value action is selected with probability $1-\\epsilon_t(s)$ when in state $s$ at time $t$. Just like for Boltzmann exploration, we must find an appropriate decay schedule to make this method into a GLIE as previously proposed. In a similar way, we need to ensure that $\\sum \\limits_{i=1}^{\\infty} Pr(a|s, t_s(i)) = \\infty$.\n",
    "\n",
    "By making $\\epsilon_t(s)=c/n_t(s)$ with $0 < c < 1$, we therefore have for all $a$, $P(a|s, t_s(i)) \\ge \\epsilon(s)/m$. Given that $\\sum_{b \\in A}^{\\infty} c / i = \\infty$, we can recognize we have the the same sum and therefore $\\sum \\limits_{i=1}^{\\infty} Pr(a|s, t_s(i)) = \\infty$.\n",
    "\n",
    "In addition, $\\lim_{n_t(s) \\rightarrow \\infty} e_t(s) = c/n_t(s) = 0$ satisfying GLIE requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
